# Distributed Music Streams Processing Lab

![System Architecture](images/system_architecture.png)

---

## ğŸ¯Â BusinessÂ Goal
Provide **nearâ€‘realâ€‘time listener & track analytics** without the complexity of a fullâ€‘blown streaming stack:

* Random userâ€‘stream events are landed as small CSV objects in an S3 *incoming* prefix.
* An **ApacheÂ Airflow** DAG checks that prefix every **5â€¯minutes**. When files exist, it kicks off processingâ€”otherwise it simply sleeps until the next tick.
* **AWSÂ GlueÂ (PySpark)** cleanses, enriches, and converts the data to columnar Parquet.
* A followâ€‘up **Glue Pythonâ€‘Shell** task aggregates and upserts metrics into **AmazonÂ DynamoDB**, where dashboards & Lambda APIs read them instantly.

This microâ€‘batch design acts "streamâ€‘like" while staying costâ€‘efficient and serverless.

---

## âš™ï¸Â PipelineÂ Steps

| # | Airflow Task  | Purpose | Success Proof |
|---|--------------|---------|----------------|
| 1 | **`check_files`** | Detect new CSVs in `s3://music-streams/incoming/`. | â€“ |
| 2 | **`glue_pyspark`** | Transform & repartition â†’ Parquet in curated S3 prefix. | ![SparkÂ Success](images/aws_glue_data_transformation_spark_job_success.png) |
| 3 | **`glue_dynamo`** | Aggregate & upsert into DynamoDB. | ![DynamoÂ Success](images/aws_glue_inserting_into_dynamodb_python_job_success.png) |
| 4 | **`move_files`** | Archive raw CSVs to `s3://music-streams/archive/DATE/`. | â€“ |

Overall DAG window:

![AirflowÂ DAG](images/airflow_dag_success.png)

Intermediate & final outputs:

![ParquetÂ Preview](images/intermediate_spark_output.png)

![DynamoÂ View](images/dynamo_db_final_output.png)

---

## ğŸ—‚ï¸Â RepositoryÂ Layout
```text
â”œâ”€ data/images/                 # PNGs & diagrams
â”œâ”€ dag-glue-workflow.py         # Airflow DAG (orchestrator)
â”œâ”€ glue-pyspark.py              # Spark ETL script
â”œâ”€ glue-dynamo.py               # DynamoDB loader script
â”œâ”€ local-docker-development.sh  # LocalStack + Airflow helper
â””â”€ README.md                    # Youâ€™re reading it âœ”ï¸
```

---

## ğŸš€Â QuickÂ Start (Local)
```bash
# Boot LocalStack + Airflow
sh local-docker-development.sh

# Upload sample CSV
aws --endpoint-url=http://localhost:4566 \
    s3 cp sample_plays.csv s3://music-streams/incoming/
```
The DAG will autoâ€‘trigger within the next 5â€‘minute cycle and push results to the local DynamoDB table.

---

