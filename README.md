# Distributed music streams processing using Airflow, Spark & Dynamodb

![System Architecture](data/images/system_architecture.png)

A **lightweight, serverlessâ€‘friendly ETL pipeline** that lands raw musicâ€‘stream CSVs in **AmazonÂ S3**, transforms them with **AWSÂ GlueÂ (PySpark)**, and publishes aggregated metrics to **AmazonÂ DynamoDB**â€”all orchestrated by a concise **ApacheÂ Airflow** DAG that runs every fiveÂ minutes.

---

## âš™ï¸  How the Pipeline Flows

| Step | Task | What Happens | Proofâ€‘ofâ€‘Life |
|------|------|--------------|---------------|
|â€¯1â€¯| `check_files` | Airflow senses new objects in the *incoming* S3 prefix. | â€“ |
|â€¯2â€¯| `glue_pyspark` | Glue Spark job cleans, enriches & repartitions data â†’ writes Parquet back to S3. | ![PySparkÂ Success](data/images/aws_glue_data_transformation_spark_job_success.png) |
|â€¯3â€¯| `glue_dynamo` | Glue Pythonâ€‘shell job aggregates & upserts records into DynamoDB. | ![DynamoÂ LoadÂ Success](data/images/aws_glue_inserting_into_dynamodb_python_job_success.png) |
|â€¯4â€¯| `move_files` | Original CSVs are archived to a dated folder. | â€“ |

A successful run in the Airflow UI looks like this:

![DAGÂ Success](data/images/airflow_dag_success.png)

Sample intermediate output and DynamoDB view:

<p float="left">
  <img src="data/images/intermediate_spark_output.png" width="48%" />
  <img src="data/images/dynamo_db_final_output.png" width="48%" />
</p>

---

## ğŸ—‚ï¸  RepoÂ Layout

```
â”œâ”€ data/images/                 # PNGs & diagrams
â”œâ”€ dag-glue-workflow.py         # Airflow DAG (main orchestrator)
â”œâ”€ glue-pyspark.py              # Spark ETL script
â”œâ”€ glue-dynamo.py               # DynamoDB loader script
â”œâ”€ local-docker-development.sh  # Oneâ€‘shot LocalStack + Airflow dev environment
â””â”€ README.md
```

---

## ğŸš€  QuickÂ Start (Local)

```bash
# spin up local Airflow + LocalStack
sh local-docker-development.sh
```
Then drop some CSVs into `s3://music-streams/incoming/` (LocalStack bucket) and watch the DAG run.

---

